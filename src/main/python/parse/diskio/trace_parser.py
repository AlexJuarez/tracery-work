from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

from collections import defaultdict
from utils import Logger
import sys
import re

#  BLW Questions/Issues:
#  What if a sector moves to an on-disk file system structure
#    (e.g., bitmap)?
#  Need to at least note that concurrent reads to the same sector(s) are
#    not handled correctly.
#  What about concurrent reads to 1,3 and to 2?
#  What about multiple writebacks to same sector(s)?
#  Can an inode get remappped between the start and end of an IO?
#    (seems like a real pain if possible)
#  We see "gather" activity for reads -- why not "scatter" activity
#    for writes?


# Parser for disk-IO trace files generated by the kprobe based profiler

# FB-specific synchronization marker
FB_SYNC_IDENTIFIER = '__FB_PERF_SYNC_MARKER__'

# Given a buffer and regex, parses the buffer into groups
# The (optional) ints array allows callers to specify positions of
# integer groups so that they can be correctly typecast before return
def parse_buffer(buffer, regex, ints):
    match = re.match(regex, buffer)
    if not match:
        raise Exception('Failed to parse buffer ' + buffer + ' (' + regex + ')')

    groups = [g for g in match.groups()]
    for i in ints:
        # Incoming ints are 1-based, but the array is 0-based
        groups[i - 1] = int(groups[i - 1])

    return tuple(groups)

def make_key(a, b, c):
    return str(a) + '-' + str(b) + '-' + str(c)

# A class to represent synchronization information exchanged between the app
# being traced and the tracer. At this time, we're only interested in the
# identifier and the sync timestamp
class AppSyncInfo(object):
    def __init__(self, identifier, timestamp):
        self.identifier = identifier
        self.timestamp = timestamp

# FIXME: Update all file ops to use FileInfo
class FileInfo(object):
    def __init__(self, inode, name, size):
        self.inode = inode
        self.name = name
        self.size = size
        self.nr_reads = 0
        self.nr_writes = 0

    def update_size(self, new_size):
        self.size = new_size

    def update_reads(self, reads):
        self.nr_reads += reads

    def update_writes(self, writes):
        self.nr_writes += writes

# Base class for IO events
class IOEvent(object):
    def __init__(self, type, thread_name, cpu, ts, file_name):
        self.type = type
        self.thread_name = thread_name
        self.cpu = cpu
        self.ts = ts
        self.file_name = file_name
        self.duration = 0

    def set_duration(self, d):
        self.duration = d

class ReadEvent(IOEvent):
    def __init__(self, thread, cpu, ts, inode, file_name, size, start,
                 count, pages):
        super(ReadEvent, self).__init__('R', thread, cpu, ts, file_name)
        self.inode = inode
        self.size = size
        self.page_start = start
        self.page_count = count
        self.pages = pages
        self.pending = count

class WriteEvent(IOEvent):
    def __init__(self, thread, cpu, ts, inode, file_name):
        super(WriteEvent, self).__init__('W', thread, cpu, ts, file_name)
        self.inode = inode
        self.pages = None

    def set_pages(self, pages):
        self.pages = pages

class SyncEvent(IOEvent):
    def __init__(self, thread, cpu, ts, file_name, start, end):
        super(SyncEvent, self).__init__('S', thread, cpu, ts, file_name)
        self.start_addr = start
        self.end_addr = end

class WritebackEvent(IOEvent):
    def __init__(self, thread, cpu, ts, device, start, end):
        super(WritebackEvent, self).__init__('B', thread, cpu, ts, '')
        self.start_sector = start
        self.end_sector = end

    def set_start_sector(self, start):
        self.start_sector = start

    def set_end_sector(self, end):
        self.end_sector = end

    def set_filename(self, filename):
        self.file_name = filename

class DiskSector(object):
    def __init__(self, device, sector):
        self.device = device
        self.sector = sector

class DiskSectorRange(object):
    def __init__(self, device, begin, end):
        self.device = device
        self.begin = begin
        self.end = end

class TraceParser:
    def __init__(self):
        self.event_handlers = defaultdict(list)
        self.read_events = defaultdict(list)
        self.write_events = {}
        self.sync_events = {}
        self.writeback_events = {}
        self.file_names = defaultdict(lambda: 'Unknown')
        self.file_info = {}
        self.sectors = defaultdict(list)
        self.complete_events = []
        self.used_sectors = defaultdict(lambda: defaultdict())
        self.app_sync_info = []

    def register(self, event, handler):
        self.event_handlers[event].append(handler)

    def unregister(self, event, handler):
        self.event_handlers[event].remove(handler)

    def unregister_all(self):
        for e, l in self.event_handlers.iteritems():
            for h in l:
                l.remove(h)

    def update_file_info(self, inode, name, size, reads=0, writes=0):
        if inode in self.file_info:
            self.file_info[inode].update_size(size)
        else:
            self.file_info[inode] = FileInfo(inode, name, size)
        self.file_info[inode].update_reads(reads)
        self.file_info[inode].update_writes(writes)

    def remap_inode_sectors(self, device, begin, num_sectors,
                            old_inode, new_inode):
        # Delete a contiguous sector span at least once sector long and
        # starting at "begin" from old_inode's sector ranges.
        end = begin + num_sectors - 1
        last_sector_remapped = begin - 1
        sector_ranges = self.sectors[old_inode]
        for i in range(len(sector_ranges)):
            # The sector range list is sorted by device and then
            # sector range start.  First we have to walk through and
            # find the ranges for the right device, then find the
            # range containing the 'begin' sector
            curr_range = sector_ranges[i]
            if curr_range.device < device:
                # haven't found the right device yet - keep walking
                continue
            elif curr_range.device > device:
                # There are no sector ranges for this device yet, or
                # we've walked through all of the device's ranges and
                # didn't find a match.  This is a terminal error.
                Logger.error('Previous sector mapping not found for sector ' +
                             str(begin) + '  - bailing')
                return
            # Found the matching device, now walking through its ranges
            elif curr_range.end < begin:
                # haven't found the right spot yet - keep walking
                continue
            elif curr_range.begin > begin:
                # We've walked past the point where we should have found
                # the right sector range.  This is a terminal error.
                Logger.error('Previous sector mapping not found for sector ' +
                             str(begin) + '  - bailing')
                return
            else:
                # found range containing 'begin' - subtract out 'begin'
                # plus any more sectors in the remapped range
                last_sector_remapped = min(end, curr_range.end)
                if curr_range.begin < begin:
                    if curr_range.end > end:
                        # need to cut a hole out of the range
                        new_range = DiskSectorRange(device, end + 1,
                                                    curr_range.end)
                        sector_ranges.insert(i, new_range)
                    curr_range.end = begin - 1
                else:
                    # range starts right at 'begin'
                    if curr_range.end > end:
                        curr_range.begin = end + 1
                    else:
                        sector_ranges.remove(curr_range)
                break
        else:
            # There are no sector ranges for this device yet, or
            # we've walked through all of the device's ranges and
            # didn't find a match.  This is a terminal error.
            Logger.error('Previous sector mapping not found for sector ' +
                         str(begin) + '  - bailing')
            return

        dev_used_sectors = self.used_sectors[device]
        for curr_sector in range(begin, last_sector_remapped + 1):
            dev_used_sectors[curr_sector] = new_inode

    def maintain_sector_mapping(self, device, begin, num_sectors, inode):
        dev_used_sectors = self.used_sectors[device]
        for curr_sector in range(begin, begin + num_sectors):
            if curr_sector in dev_used_sectors:
                if dev_used_sectors[curr_sector] == inode:
                    continue
                else:
                    # at some point it might be necessary to use -1 to
                    # denote a sector mapped to an unknown inode...
                    if dev_used_sectors[curr_sector] >= 0:
                        Logger.warning(
                            "Sector {0}/{1:d} remapped:  inode {2:d} --> {3:d}".
                            format(device, curr_sector, 
                                   dev_used_sectors[curr_sector], inode))
                    self.remap_inode_sectors(device, curr_sector,
                                             begin + num_sectors -
                                             curr_sector,
                                             dev_used_sectors[curr_sector],
                                             inode)
            else:
                dev_used_sectors[curr_sector] = inode

    # Create an inode <-> dev/physical-sector-range mapping.
    def set_inode(self, device, begin, num_sectors, inode):
        self.maintain_sector_mapping(device, begin, num_sectors, inode)
        end = begin + num_sectors - 1
        current_position = 0
        sector_ranges = self.sectors[inode]
        for i in range(len(sector_ranges)):
            # The sector range list is sorted by device and then
            # sector range start.  First we have to walk through and
            # find the set of ranges for the right device, then find
            # the spot where we should either insert the new range or
            # coalesce it with an existing range that it is adjacent
            # to or overlapping with.
            current_position = i
            curr_range = sector_ranges[i]
            if curr_range.device < device:
                # haven't found the right device yet - keep walking
                continue
            elif curr_range.device > device:
                # There are no sector ranges for this device yet, or
                # we've walked through all of the device's ranges and
                # didn't find an insertion point.  In either case,
                # insert the new range now.
                new_range = DiskSectorRange(device, begin, end)
                sector_ranges.insert(i, new_range)
                break
            # Found the matching device, now walking through its ranges
            elif (curr_range.end + 1) < begin:
                # haven't found the right spot yet - keep walking
                continue
            elif curr_range.begin > (end + 1):
                # Walked beyond the right insertion point or any
                # possible coalescing opportunity, so insert the new
                # range now
                new_range = DiskSectorRange(device, begin, end)
                sector_ranges.insert(i, new_range)
                break
            else:
                # overlapping or adjacent ranges - coalesce 'em!
                curr_range.begin = min(curr_range.begin, begin)
                if end > curr_range.end:
                    curr_range.end = end
                    # we are stretching the existing range forward,
                    # so we need to check for any coalescing that is
                    # possible with later ranges
                    while current_position < (len(sector_ranges) - 1):
                        curr_range = sector_ranges[current_position]
                        next_range = sector_ranges[current_position + 1]
                        if (curr_range.end + 1) >= next_range.begin:
                            curr_range.end = max(curr_range.end, next_range.end)
                            sector_ranges.remove(next_range)
                        else:
                            break
                break
        else:
            # append to end of list
            new_range = DiskSectorRange(device, begin, end)
            sector_ranges.append(new_range)

    # Given a disk device and sector range, returns first matching
    #   inode and the last contiguous sector matched for that inode
    # FIXME:  This is a really simplistic (and slow) way of looking
    #         up the inode - a linear search. We could do better
    # FIXME:  Dump out a Logger message if we have one or more unmapped
    #         sectors in the range?
    def get_next_inode(self, device, begin, num_sectors):
        end = begin + num_sectors - 1
        for inode, one_inode_sector_ranges in self.sectors.iteritems():
            for curr_range in one_inode_sector_ranges:
                # The sector range list is sorted by device and then
                # sector range start.  First we have to walk through and
                # find the set of ranges for the right device, then find
                # a match for 'begin' (and then some, hopefully).
                if curr_range.device < device:
                    # haven't found the right device yet - keep walking
                    continue
                elif curr_range.device > device:
                    # There are no sector ranges for this device, or
                    # we've walked through all of the device's ranges
                    # and didn't find a match for 'begin'
                    break
                # Found the matching device, now walking through its
                # ranges
                elif curr_range.end < begin:
                    # haven't found the right match yet - keep walking
                    continue
                elif curr_range.begin > begin:
                    # We've walked past the point where we should have
                    # found the right sector range.
                    break
                else:
                    # found range containing 'begin', so return inode
                    # and the last matching sector (i.e., there may
                    # be another inode corresponding to later sectors
                    # in [begin,end]
                    return (inode, min(end, curr_range.end))
        # no inode mapping found for the 'begin' sector
        return (0, begin)

    # An inode has been remapped, so remove all existing inode --> ***
    # mappings.  Note that this leaves the physical sector --> inode
    # mapping in place, so a subsequent use of those sectors will show
    # up as a remapped sector
    def purge_inode_mappings(self, inode):
        if inode in self.file_names:
            del self.file_names[inode]
        if inode in self.file_info:
            del self.file_info[inode]
        if inode in self.sectors:
            del self.sectors[inode]

    def check_for_inode_reuse(self, inode, file_name):
        if inode in self.file_names and self.file_names[inode] != file_name:
            Logger.warning('Inode ' + str(inode) + ' reused:  ' +
                           self.file_names[inode] + ' --> ' + file_name)
            self.purge_inode_mappings(inode)


    def add_read_event(self, event):
        self.read_events[event.inode].append(event)

    # returns event corresponding to given inode and start + count tuple
    # note that we can't just look at start + count to determine if
    # the given input tuple is within a previously seen range. This is
    # because # reads are not always continguous. For example, we could
    # have:
    # read-issue: 1, 2, 4, 5
    # read-return: start 1, count 2
    # read-return: start 4, count 2
    # In this case, if we only matched by start + count, the second
    # return would fall out of range. Instead, since we already track
    # the list of pages for which a read issued, we just look at the
    # last item in the list to determine the upper bound of the range
    # (list is always sorted)
    def get_read_event(self, inode, start, count):
        events = self.read_events[inode]

        for event in events:
            # start + count - 1 because event.pages is 0-based, count is not
            if start >= event.page_start and \
                    (start + count - 1) <= event.pages[-1]:
                return event

        return None

    def delete_read_event(self, event):
        self.read_events[event.inode].remove(event)

    # /data/app/com.facebook.katana-1.apk ino 773682 size 1024 count 2
    # range 10186-10187 pages 10186 10187
    def handleReadBegin(self, thread_name, cpu, usec, details):
        regex = ('\\s*(\\S+) ino (\\d+) size (\\d+) count (\\d+)' +
                 ' range (\\d+)-(\\d+) pages (.*)$')

        ints = [2, 3, 4, 5, 6]
        file_name, inode, size, count, start, end, pages = \
            parse_buffer(details, regex, ints)

        self.check_for_inode_reuse(inode, file_name)
        self.file_names[inode] = file_name

        regex = ''
        ints = []
        for i in range(0, int(count)):
            regex = regex + '(\\d+)\\s*'
            ints.append(i)
        pages = list(parse_buffer(pages, regex, ints))

        event = ReadEvent(thread_name, cpu, usec, inode, file_name,
                          size, int(start), int(count), pages)
        self.add_read_event(event)

        return True

    # read ino 773682 start 10186 count 32
    def handleReadEnd(self, thread_name, cpu, usec, details):
        regex = ('(\\S+) ino (\\d+) start (\\d+) count (\\d+)')

        ints = [2, 3, 4]
        t, inode, start, count = parse_buffer(details, regex, ints)
        event = self.get_read_event(inode, start, count)
        if not event:
            # not catastrophic, but we can't do much more
            Logger.warning('Unmatched read-end seen: ' + details)
            return True

        self.update_file_info(inode, event.file_name, event.size, reads=count)

        # Read events are interesting: for large reads, the ext4 layer
        # combines multiple (possibly non-contiguous) pages from the
        # same inode into a single read request and issues it to the
        # block driver. The block driver breaks down the request into
        # multiple contiguous pages, reads them and returns them
        # asynchronously out-of-order. So, in order to track a read
        # request accurately, we need to track all the completion events
        # returned by the block driver and add them up and mark the read
        # as complete after all pages have been returned to the
        # application.
        event.pending = event.pending - count
        if event.pending < 0:
            Logger.warning('Pending read event underflow')
            event.pending = 0

        if event.pending == 0:
            # This read is complete
            event.set_duration(usec - event.ts)
            self.delete_read_event(event)
            self.complete_events.append(event)

        return True

    # /data/data/com.facebook.katana/app_state_logs/
    # 14be86c7-5bb8-79c6-36bc-af1abc51845f.txt ino 579040 size 258
    # pos 30 len 1
    def handleWriteBegin(self, thread_name, cpu, usec, details):
        regex = ('(\\S+).*ino (\\d+) size (\\d+) pos (\\d+) len (\\d+)')
        
        ints = [2, 3, 4, 5]
        file_name, inode, size, pos, len = parse_buffer(details, regex, ints)

        self.check_for_inode_reuse(inode, file_name)
        self.file_names[inode] = file_name

        event = WriteEvent(thread_name, cpu, usec, inode, file_name)
        key = make_key(inode, pos, len)
        self.write_events[key] = event

        return True

    # /data/data/com.facebook.katana/app_state_logs/
    # 14be86c7-5bb8-79c6-36bc-af1abc51845f.txt
    # ino 579040 size 258 pos 30 len 1 page 0
    def handleWriteEnd(self, thread_name, cpu, usec, details):
        regex = ('(\\S+).*ino (\\d+) size (\\d+) pos (\\d+) len (\\d+)' +
                 ' page (\\d+)')
        
        ints = [2, 3, 4, 5, 6]
        file_name, inode, size, pos, len, page = \
            parse_buffer(details, regex, ints)
        key = make_key(inode, pos, len)

        if key not in self.write_events:
            Logger.warning('Unmatched write-end seen: ' + details)
            return True

        event = self.write_events[key]

        event.set_pages([page])
        event.set_duration(usec - event.ts)
        self.write_events.pop(key, None)
        self.complete_events.append(event)

        # Current assumption is these calls will always be single page
        # GMurthy:  That's because the write methods we're tracing in the
        # kernel appear to work at the granularity of 1 page, and
        # no more -- unless I'm reading the code wrong. See for
        # reference: Write-begin:
        # http://lxr.free-electrons.com/source/fs/ext4/inode.c#L2703
        # Write-end: http://lxr.free-electrons.com/source/fs/ext4/inode.c#L2823
        self.update_file_info(inode, file_name, size, writes=1)

        return True

    # /data/data/com.facebook.katana/databases/
    # prefs_db-journal start 1615706553 end 4294967295
    def handleSyncBegin(self, thread_name, cpu, usec, details):
        regex = ('(\\S+).*start (\\d+) end (\\d+)')

        ints = [2, 3]
        file_name, start, end = parse_buffer(details, regex, ints)
        event = SyncEvent(thread_name, cpu, usec, file_name, start, end)
        self.sync_events[thread_name] = event

        return True

    #  retval 0 lat 30
    def handleSyncEnd(self, thread_name, cpu, usec, details):
        if thread_name not in self.sync_events:
            Logger.warning('Unmatched sync-end seen: ' + details)
            return True

        event = self.sync_events[thread_name]
        event.set_duration(usec - event.ts)
        self.sync_events.pop(thread_name, None)
        self.complete_events.append(event)

        return True

    # ino 529428 dev 179,0 W sector 19478088 size 8
    def handleSubmitBio(self, thread_name, cpu, usec, details):
        regex = ('ino (\\d+) dev (\\d+,\\d+) \\S+ sector (\\d+) size (\\d+)')

        ints = [1, 3, 4]
        inode, device, sector, num_sectors = parse_buffer(details, regex, ints)
        self.set_inode(device, sector, num_sectors, inode)

        return True

    # write-backs are interesting as well, because we don't have a
    # convenient completion handler like we do for reads. We have
    # block_rq_issue/complete but they're at the block level and as such,
    # only deal with sectors. This makes it hard to map it back to
    # user-space files. Fortunately, we have an instrumentation point
    # (submit_bio) that allows us to map sector addresses to inodes,
    # and we already map inodes to filenames. So here, we just build
    # the right mappings and attribute write-backs to files note:
    # write-backs due to a sync typically don't carry inode info

    # 179,0 WS 0 () 19478136 + 8 [mmcqd/0]
    def handleWritebackBegin(self, thread_name, cpu, usec, details):
        regex = ('(\\d+,\\d+) F?([DWRN])F?A?S?M? ' +
                 '\\d+ \\(.*\\) (\\d+) \\+ (\\d+) \\[.*\\]')

        ints = [3, 4]
        device, op, sector, num_sectors = parse_buffer(details, regex, ints)
        if op == 'W' and len(self.sectors) > 0:
            key = make_key(device, sector, num_sectors)
            event = WritebackEvent(thread_name, cpu, usec, device, sector,
                                   sector + num_sectors)
            self.writeback_events[key] = event
        return True

    # 179,0 WS 0 () 19478136 + 8 [0]
    def handleWritebackEnd(self, thread_name, cpu, usec, details):
        regex = ('(\\d+,\\d+) F?([DWRN])F?A?S?M? ' +
                 '\\(.*\\) (\\d+) \\+ (\\d+) \\[.*\\]')
        
        ints = [3, 4]
        device, op, sector, num_sectors = parse_buffer(details, regex, ints)
        if op == 'W' and len(self.sectors) > 0:
            key = make_key(device, sector, num_sectors)
            if key not in self.writeback_events:
                Logger.warning('Unmatched writeback-end seen: ' + details)
                return True
            event = self.writeback_events[key]
            self.writeback_events.pop(key, None)
            event.set_duration(usec - event.ts)
            last_sector_mapped = sector - 1
            while last_sector_mapped < (sector + num_sectors - 1):
                inode, last_sector_mapped = self.get_next_inode(device, 
                    last_sector_mapped + 1,
                    sector + num_sectors - last_sector_mapped - 1)
                event.set_end_sector(last_sector_mapped + 1)
                file_name = self.file_names[inode]
                event.set_filename(file_name)
                self.complete_events.append(event)
                if last_sector_mapped < sector + num_sectors - 1:
                    # this is not the last piece of the request
                    event = WritebackEvent(thread_name, cpu, event.ts,
                                           device, sector, sector + num_sectors)
                    event.set_duration(usec - event.ts)
                    event.set_start_sector(last_sector_mapped + 1)
        return True

    # facebook.katana-4099  [000]   952.907684: 0: __FB_PERF_SYNC_MARKER__
    # Applications can choose to synchronize with the profiler by emitting
    # a special type of event into the trace, such as the one shown above.
    # Such synchronizations could be useful for, among other things:
    # a. establishing a temporal ordering of interesting events
    # b. synchronizing clock differences between the app and the profiler
    # Currently, the Facebook apps emit sync events for (b)
    def handleAppSynchronization(self, thread_name, cpu, usec, details):
        regex = ('(\\w+)')
        sync_string, = parse_buffer(details, regex, [])
        if sync_string == FB_SYNC_IDENTIFIER:
            self.app_sync_info.append(AppSyncInfo(sync_string, usec))

    # Parses trace file at given path and returns a list of events
    def parse_trace(self, path):
        self.register('jext4_readpages', self.handleReadBegin)
        self.register('jext4_readpage', self.handleReadBegin)
        self.register('jmpage_end_io', self.handleReadEnd)
        self.register('jext4_da_write_begin', self.handleWriteBegin)
        self.register('jext4_da_write_end', self.handleWriteEnd)
        self.register('jext4_sync_file', self.handleSyncBegin)
        self.register('ret_handler', self.handleSyncEnd)
        self.register('jsubmit_bio', self.handleSubmitBio)
        self.register('block_rq_issue', self.handleWritebackBegin)
        self.register('block_rq_complete', self.handleWritebackEnd)
        self.register('0', self.handleAppSynchronization)

        self.process_trace(path)

        self.unregister_all()
        return (self.complete_events, self.file_info.values())

    def get_app_sync_info(self):
        return self.app_sync_info

    def process_trace(self, path):
        #  Android Linux kernel 3.0 and earlier default to the following ftrace format:
        # # tracer: nop
        # #
        # #           TASK-PID    CPU#    TIMESTAMP  FUNCTION
        # #              | |       |          |         |
        #  UsageStatsServi-4228  [000] 10663.962738: jext4_da_write_begin: /data/system/usagestats/usage-20160331 ino 529428 size 0 pos 0 len 504
        # ...
        TRACE_RE = '\\s*(\\S+)\\s*\[(\\S+)\]\\s*(\\d+)\.(\\d+): (\\S+): (.*$)'

        # Android Linux kernel 3.3 and later default to the following ftrace format:
        # # tracer: nop
        # #
        # # entries-in-buffer/entries-written: 18699/60422   #P:1
        # #
        # #                              _-----=> irqs-off
        # #                             / _----=> need-resched
        # #                            | / _---=> hardirq/softirq
        # #                            || / _--=> preempt-depth
        # #                            ||| /     delay
        # #           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION
        # #              | |       |   ||||       |         |
        #            <...>-1363  [003] d..1    85.132172: jext4_da_write_begin: /data/data/com.facebook.wakizashi/databases/newsfeed_db ino 417053 size 172032 pos 139264 len 4096
        # ...
        TRACE_RE_IRQINFO = '\\s*(\\S+)\\s*\[(\\S+)\]\\s*\\S+\\s*(\\d+)\.(\\d+): (\\S+): (.*$)'

        # Android Linux kernel 3.10 and later extend the above formats with an optional TGID column.
        # These and other non-default output configurations are not currently supported.

        with open(path, 'r') as f:
            # Determine trace format (new or old).
            regex = None
            for line in f:
                if regex is not None:
                    match = regex.match(line)
                    if not match:
                        continue

                    thread_name = match.group(1)
                    cpu = int(match.group(2))
                    sec = int(match.group(3))
                    usec = int(match.group(4))
                    event = match.group(5)
                    details = match.group(6)

                    for handler in self.event_handlers[event]:
                        ret = handler(thread_name, cpu, sec * 1e6 + usec, details)
                        if not ret:
                            Logger.error('Handler failed - bailing')
                            break
                else:
                    if line.startswith('#           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION'):
                        regex = re.compile(TRACE_RE_IRQINFO)
                    elif line.startswith('#           TASK-PID    CPU#    TIMESTAMP  FUNCTION'):
                        regex = re.compile(TRACE_RE)
                    elif line.startswith('#           TASK-PID'):
                        Logger.error('Unsupported ftrace format  - bailing')
                        return

if __name__ == "__main__":
    parser = TraceParser()
    events, files = parser.parse_trace('./example_trace')
    for event in events:
        print(event.type + ' ' + str(event.ts) + ' ' + event.file_name +
              ' ' + str(event.duration))
